{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "# import sys\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "random.seed(49999)\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy\n",
    "numpy.random.seed(49999)\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(49999)\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import inputs\n",
    "import metrics\n",
    "from losses import *\n",
    "from optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model specific imports\n",
    "from models.model import BasicModel\n",
    "from layers.DynamicMaxPooling import *\n",
    "from utils.utility import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossATT(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, c_maxlen, q_maxlen, dropout, **kwargs):\n",
    "        self.output_dim=output_dim\n",
    "        self.c_maxlen = c_maxlen\n",
    "        self.q_maxlen = q_maxlen\n",
    "        self.dropout = dropout\n",
    "        super(CrossATT, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(None, ?, 128), (None, ?, 128)]\n",
    "        init = VarianceScaling(scale=1.0, mode='fan_in', distribution='normal')\n",
    "        self.W0 = self.add_weight(name='W0',\n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W1 = self.add_weight(name='W1',\n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "\n",
    "        super(CrossATT, self).build(input_shape)\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value = -1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x_cont, x_ques, c_mask, q_mask = x\n",
    "        S = K.batch_dot(x_cont, K.permute_dimensions(x_ques, pattern=(0, 2, 1)))\n",
    "        S_ = tf.nn.softmax(S)\n",
    "        S_n = tf.expand_dims(S_, 3)\n",
    "        vs = K.tile(S_n, [1, 1, 1, self.output_dim])\n",
    "        v0 = tf.expand_dims(x_ques, 1)\n",
    "        v1 = K.tile(v0, [1, self.c_maxlen, 1, 1])\n",
    "        c2q = tf.multiply(vs, v1)\n",
    "        v11 = K.sum(c2q, axis=2)\n",
    "        v2 = K.dot(v11, self.W1)\n",
    "        v3 = K.dot(x_cont, self.W0)\n",
    "        result = v2 + v3\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "class context2query_attention(Layer):\n",
    "\n",
    "    def __init__(self, output_dim, c_maxlen, q_maxlen, dropout, **kwargs):\n",
    "        self.output_dim=output_dim\n",
    "        self.c_maxlen = c_maxlen\n",
    "        self.q_maxlen = q_maxlen\n",
    "        self.dropout = dropout\n",
    "        super(context2query_attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # input_shape: [(None, ?, 128), (None, ?, 128)]\n",
    "        init = VarianceScaling(scale=1.0, mode='fan_in', distribution='normal')\n",
    "        self.W0 = self.add_weight(name='W0',\n",
    "                                  shape=(input_shape[0][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W1 = self.add_weight(name='W1',\n",
    "                                  shape=(input_shape[1][-1], 1),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.W2 = self.add_weight(name='W2',\n",
    "                                  shape=(1, 1, input_shape[0][-1]),\n",
    "                                  initializer=init,\n",
    "                                  regularizer=l2(3e-7),\n",
    "                                  trainable=True)\n",
    "        self.bias = self.add_weight(name='linear_bias',\n",
    "                                    shape=([1]),\n",
    "                                    initializer='zero',\n",
    "                                    regularizer=l2(3e-7),\n",
    "                                    trainable=True)\n",
    "        super(context2query_attention, self).build(input_shape)\n",
    "\n",
    "    def mask_logits(self, inputs, mask, mask_value = -1e30):\n",
    "        mask = tf.cast(mask, tf.float32)\n",
    "        return inputs + mask_value * (1 - mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        x_cont, x_ques, c_mask, q_mask = x\n",
    "\n",
    "        # get similarity matrix S\n",
    "        subres0 = K.tile(K.dot(x_cont, self.W0), [1, 1, self.q_maxlen])\n",
    "        subres1 = K.tile(K.permute_dimensions(K.dot(x_ques, self.W1), pattern=(0, 2, 1)), [1, self.c_maxlen, 1])\n",
    "        subres2 = K.batch_dot(x_cont * self.W2, K.permute_dimensions(x_ques, pattern=(0, 2, 1)))\n",
    "        S = subres0 + subres1 + subres2\n",
    "        S += self.bias\n",
    "        q_mask = tf.expand_dims(q_mask, 1)\n",
    "        S_ = tf.nn.softmax(self.mask_logits(S, q_mask))\n",
    "        c_mask = tf.expand_dims(c_mask, 2)\n",
    "        S_T = K.permute_dimensions(tf.nn.softmax(self.mask_logits(S, c_mask), axis=1), (0, 2, 1))\n",
    "        c2q = tf.matmul(S_, x_ques)\n",
    "        q2c = tf.matmul(tf.matmul(S_, S_T), x_cont)\n",
    "        result = K.concatenate([x_cont, c2q, x_cont * c2q, x_cont * q2c], axis=-1)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayer, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # assert len(input_shape) == 3\n",
    "        print(\"input_shape\", input_shape)\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # size of x :[batch_size, sel_len, attention_dim]\n",
    "        # size of u :[batch_size, attention_dim]\n",
    "        # uit = tanh(xW+b)\n",
    "        # print(\"x\", x.get_shape().as_list())\n",
    "        # print(\"W\", self.W.get_shape().as_list())\n",
    "        # print(\"b\", self.b.get_shape().as_list())\n",
    "        # print(\"u\", self.u.get_shape().as_list())\n",
    "        # # v1 = tf.matmul(x, self.W)\n",
    "        # W = K.variable(self.init((x.get_shape().as_list()[-1], self.attention_dim)))\n",
    "        # print(\"W1\", W.get_shape().as_list())\n",
    "        v1 = K.dot(x, self.W)\n",
    "        v2 = K.bias_add(v1, self.b)\n",
    "        uit = K.tanh(v2)\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        # 自然对数为底的指数\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        # ait是概率List\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return output\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])\n",
    "    \n",
    "class AttLayernew(Layer):\n",
    "    def __init__(self, attention_dim):\n",
    "        self.init = initializers.get('normal')\n",
    "        self.supports_masking = True\n",
    "        self.attention_dim = attention_dim\n",
    "        super(AttLayernew, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # assert len(input_shape) == 3\n",
    "        print(\"input_shape\", input_shape)\n",
    "        self.W = K.variable(self.init((input_shape[-1], self.attention_dim)))\n",
    "        self.b = K.variable(self.init((self.attention_dim, )))\n",
    "        self.u = K.variable(self.init((self.attention_dim, 1)))\n",
    "        self.trainable_weights = [self.W, self.b, self.u]\n",
    "        super(AttLayernew, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return mask\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        v1 = K.dot(x, self.W)\n",
    "        v2 = K.bias_add(v1, self.b)\n",
    "        uit = K.tanh(v2)\n",
    "        ait = K.dot(uit, self.u)\n",
    "        ait = K.squeeze(ait, -1)\n",
    "        ait = K.exp(ait)\n",
    "\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            ait *= K.cast(mask, K.floatx())\n",
    "        ait /= K.cast(K.sum(ait, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        ait = K.expand_dims(ait)\n",
    "        # ait是概率List\n",
    "        weighted_input = x * ait\n",
    "        output = K.sum(weighted_input, axis=1)\n",
    "        return [ait, output]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(input_shape[0], input_shape[1]), (input_shape[0], input_shape[-1])]\n",
    "\n",
    "class TileLayer(Layer):\n",
    "    def __init__(self, dim):\n",
    "        self.dim = dim\n",
    "        super(TileLayer, self).__init__()\n",
    "\n",
    "    def call(self, q_embed, mask=None):\n",
    "        q_emb_exp = K.expand_dims(q_embed, axis=1)\n",
    "        show_layer_info('exp 1', q_emb_exp)\n",
    "        q_emb_reshape = K.tile(q_emb_exp, (1, self.dim, 1))\n",
    "        show_layer_info('tile 1', q_emb_reshape)\n",
    "        return q_emb_reshape\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.dim, input_shape[1])\n",
    "\n",
    "\n",
    "class SqueezeLayer(Layer):\n",
    "    def __init__(self, dim):\n",
    "        # self.init = initializers.get('normal')\n",
    "        # self.supports_masking = True\n",
    "        self.dim = dim\n",
    "        super(SqueezeLayer, self).__init__()\n",
    "\n",
    "    def call(self, q_embed, mask=None):\n",
    "        q_emb_exp = K.squeeze(q_embed, axis=self.dim)\n",
    "        # q_emb_exp = K.squeeze(q_embed)\n",
    "        show_layer_info('squeeze 1', q_emb_exp)\n",
    "        return q_emb_exp\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[2], input_shape[3])\n",
    "\n",
    "class MYMODEL(BasicModel):\n",
    "    def __init__(self, config):\n",
    "        super(MYMODEL, self).__init__(config)\n",
    "        self.__name = 'MYMODEL'\n",
    "        self.check_list = [ 'text1_maxlen', 'text2_maxlen',\n",
    "                   'embed', 'embed_size', 'train_embed',  'vocab_size',\n",
    "                   'hidden_size', 'topk', 'dropout_rate']\n",
    "        self.embed_trainable = config['train_embed']\n",
    "\n",
    "        self.setup(config)\n",
    "        if not self.check():\n",
    "            raise TypeError('[MYMODEL] parameter check wrong')\n",
    "        self.sent_num = int(self.config['text2_maxlen']/self.config['text1_maxlen'])\n",
    "        print('[MYMODEL] init done', end='\\n')\n",
    "\n",
    "    def setup(self, config):\n",
    "        if not isinstance(config, dict):\n",
    "            raise TypeError('parameter config should be dict:', config)\n",
    "\n",
    "        self.set_default('hidden_size', 32)\n",
    "        self.set_default('topk', 100)\n",
    "        self.set_default('dropout_rate', 0)\n",
    "        self.config.update(config)\n",
    "\n",
    "    def build(self):\n",
    "\n",
    "        query = Input(name='query', shape=(self.config['text1_maxlen'],))\n",
    "        show_layer_info('Input', query)\n",
    "        doc = Input(name='doc', shape=(self.config['text2_maxlen'],))\n",
    "        show_layer_info('Input', doc)\n",
    "        sent = Input(shape=(self.config['text1_maxlen'],))\n",
    "\n",
    "        embedding = Embedding(self.config['vocab_size'], self.config['embed_size'], weights=[self.config['embed']], trainable = self.embed_trainable)\n",
    "        q_embed = embedding(query)\n",
    "        show_layer_info('Embedding', q_embed)\n",
    "        # d_embed = embedding(doc)\n",
    "        # show_layer_info('Embedding', d_embed)\n",
    "        s_embed = embedding(sent)\n",
    "        show_layer_info('Embedding', s_embed)\n",
    "\n",
    "        q_rep = Bidirectional(LSTM(self.config['hidden_size'], return_sequences=True, dropout=self.config['dropout_rate']))(q_embed)\n",
    "        show_layer_info('Bidirectional-LSTM', q_rep)\n",
    "        # d_rep = Bidirectional(GRU(self.config['hidden_size'], return_sequences=True, dropout=self.config['dropout_rate']))(d_embed)\n",
    "        # show_layer_info('Bidirectional-LSTM', d_rep)\n",
    "        s_rep = Bidirectional(LSTM(self.config['hidden_size'], return_sequences=True, dropout=self.config['dropout_rate']))(s_embed)\n",
    "        show_layer_info('Bidirectional-LSTM', s_rep)\n",
    "\n",
    "\n",
    "        c_mask = Lambda(lambda x: tf.cast(x, tf.bool))(doc) # [bs, c_len]\n",
    "        q_mask = Lambda(lambda x: tf.cast(x, tf.bool))(query)\n",
    "        s_mask = Lambda(lambda x: tf.cast(x, tf.bool))(sent)\n",
    "        # cont_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(c_mask)\n",
    "        # ques_len = Lambda(lambda x: tf.expand_dims(tf.reduce_sum(tf.cast(x, tf.int32), axis=1), axis=1))(q_mask)\n",
    "        x = context2query_attention(8 * self.config['hidden_size'], self.config['text1_maxlen'], self.config['text1_maxlen'], self.config['dropout_rate'])([s_rep, q_rep, s_mask, q_mask])\n",
    "        # x = CrossATT(100, self.config['text1_maxlen'], self.config['text1_maxlen'], self.config['dropout_rate'])([s_rep, q_rep, s_mask, q_mask])\n",
    "        show_layer_info('context2query_attention', x)\n",
    "        \n",
    "#         l_att = AttLayer(2 * self.config['hidden_size'])(x)\n",
    "        l_att = AttLayer(2 * self.config['hidden_size'])(x)\n",
    "        show_layer_info('att 1', l_att)\n",
    "        sentEncoder = Model([sent, query], l_att)\n",
    "\n",
    "        query4 = TileLayer(self.sent_num)(query)\n",
    "        query4_s = Reshape((self.sent_num, self.config['text1_maxlen']))(query4)\n",
    "        show_layer_info('query4_s', query4_s)\n",
    "        doc4 = Reshape((self.sent_num, self.config['text1_maxlen']))(doc)\n",
    "        show_layer_info('doc4', doc4)\n",
    "\n",
    "        concat = concatenate([query4_s, doc4])\n",
    "        show_layer_info('concat 1', concat)\n",
    "        out_model = TimeDistributed(Lambda(lambda x: sentEncoder([x[:,:self.config['text1_maxlen']], x[:, self.config['text1_maxlen']:]])))(concat)\n",
    "        l_att_sent_weights, l_att_sent = AttLayernew(2 * self.config['hidden_size'])(out_model)\n",
    "        show_layer_info('att 2', l_att_sent)\n",
    "        s_att_d = Dense(2 * self.config['hidden_size'], activation='relu')(l_att_sent)\n",
    "\n",
    "        q_att = AttLayer(2 * self.config['hidden_size'])(q_rep)\n",
    "        show_layer_info('att q', q_att)\n",
    "        cross = multiply([q_att, s_att_d])\n",
    "        # -1 flatten \n",
    "        cross_reshape = Reshape((-1, ))(cross)\n",
    "        show_layer_info('Reshape', cross_reshape)\n",
    "\n",
    "        # mm_k = Lambda(lambda x: K.tf.nn.top_k(x, k=self.config['topk'], sorted=True)[0])(cross_reshape)\n",
    "        # show_layer_info('Lambda-topk', mm_k)\n",
    "\n",
    "        pool1_flat_drop = Dropout(rate=self.config['dropout_rate'])(cross_reshape)\n",
    "        show_layer_info('Dropout', pool1_flat_drop)\n",
    "\n",
    "        if self.config['target_mode'] == 'classification':\n",
    "            out_ = Dense(2, activation='softmax')(pool1_flat_drop)\n",
    "        elif self.config['target_mode'] in ['regression', 'ranking']:\n",
    "            #temp1 = Dense(self.config['hidden_size']//2, activation='relu')(pool1_flat_drop)\n",
    "            #temp2 = Dense(self.config['hidden_size']//10, activation='relu')(temp1)\n",
    "            out_ = Dense(1)(pool1_flat_drop)\n",
    "        show_layer_info('Dense', out_)\n",
    "\n",
    "        #model = Model(inputs=[query, doc, dpool_index], outputs=out_)\n",
    "        model = Model(inputs=[query, doc], outputs=[out_, l_att_sent_weights])\n",
    "#         model = Model(inputs=[query, doc], outputs=out_)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_hinge_loss(kwargs=None):\n",
    "    margin = 1.\n",
    "    if isinstance(kwargs, dict) and 'margin' in kwargs:\n",
    "        margin = kwargs['margin']\n",
    "\n",
    "    def _margin_loss(y_true, y_pred):\n",
    "        # output_shape = K.int_shape(y_pred)\n",
    "        y_pos = Lambda(lambda a: a[::2, :], output_shape= (1,))(y_pred)\n",
    "        y_neg = Lambda(lambda a: a[1::2, :], output_shape= (1,))(y_pred)\n",
    "        loss = K.maximum(0., margin + y_neg - y_pos)\n",
    "        return K.mean(loss)\n",
    "    return _margin_loss\n",
    "\n",
    "\n",
    "def hinge_loss_over_sentence(num_sent, margin=0.1):\n",
    "    def _margin_loss_sent(y_true, y_pred):\n",
    "        # sentence loss only over positive samples\n",
    "        y_pos = Lambda(lambda a: a[::2, :1], output_shape= (1,))(y_pred)\n",
    "        y_neg = Lambda(lambda a: a[::2, 1:], output_shape= (num_sent - 1,))(y_pred)\n",
    "        y_neg_sum = K.sum(y_neg, axis=-1, keepdims=False)\n",
    "        show_layer_info('y pos', y_pos)\n",
    "        show_layer_info('y neg', y_neg)\n",
    "        show_layer_info('y neg sum', y_neg_sum)\n",
    "        loss = K.maximum(0., margin + y_neg_sum - y_pos)\n",
    "        return K.mean(loss)\n",
    "    return _margin_loss_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(config):\n",
    "    model_config = config['model']['setting']\n",
    "    model_config.update(config['inputs']['share'])\n",
    "    model = MYMODEL(model_config)\n",
    "    mo = model.build()\n",
    "    return mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(config):\n",
    "\n",
    "    print(json.dumps(config, indent=2), end='\\n')\n",
    "    # read basic config\n",
    "    global_conf = config[\"global\"]\n",
    "#     optimizer = global_conf['optimizer']\n",
    "#     optimizer = optimizers.get(optimizer)\n",
    "#     K.set_value(optimizer.lr, global_conf['learning_rate'])\n",
    "    weights_file = str(global_conf['weights_file']) + '.%d'\n",
    "    display_interval = int(global_conf['display_interval'])\n",
    "    num_iters = int(global_conf['num_iters'])\n",
    "    save_weights_iters = int(global_conf['save_weights_iters'])\n",
    "\n",
    "    # read input config\n",
    "    input_conf = config['inputs']\n",
    "    share_input_conf = input_conf['share']\n",
    "\n",
    "\n",
    "    # collect embedding\n",
    "    if 'embed_path' in share_input_conf:\n",
    "        embed_dict = read_embedding(filename=share_input_conf['embed_path'])\n",
    "        _PAD_ = share_input_conf['vocab_size'] - 1\n",
    "        embed_dict[_PAD_] = np.zeros((share_input_conf['embed_size'], ), dtype=np.float32)\n",
    "        embed = np.float32(np.random.uniform(-0.2, 0.2, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = convert_embed_2_numpy(embed_dict, embed = embed)\n",
    "    else:\n",
    "        embed = np.float32(np.random.uniform(-0.2, 0.2, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = embed\n",
    "    print('[Embedding] Embedding Load Done.', end='\\n')\n",
    "\n",
    "    # list all input tags and construct tags config\n",
    "    input_train_conf = OrderedDict()\n",
    "    input_eval_conf = OrderedDict()\n",
    "    for tag in input_conf.keys():\n",
    "        if 'phase' not in input_conf[tag]:\n",
    "            continue\n",
    "        if input_conf[tag]['phase'] == 'TRAIN':\n",
    "            input_train_conf[tag] = {}\n",
    "            input_train_conf[tag].update(share_input_conf)\n",
    "            input_train_conf[tag].update(input_conf[tag])\n",
    "        elif input_conf[tag]['phase'] == 'EVAL':\n",
    "            input_eval_conf[tag] = {}\n",
    "            input_eval_conf[tag].update(share_input_conf)\n",
    "            input_eval_conf[tag].update(input_conf[tag])\n",
    "    print('[Input] Process Input Tags. %s in TRAIN, %s in EVAL.' % (input_train_conf.keys(), input_eval_conf.keys()), end='\\n')\n",
    "\n",
    "    # collect dataset identification\n",
    "    dataset = {}\n",
    "    for tag in input_conf:\n",
    "        if tag != 'share' and input_conf[tag]['phase'] == 'PREDICT':\n",
    "            continue\n",
    "        if 'text1_corpus' in input_conf[tag]:\n",
    "            datapath = input_conf[tag]['text1_corpus']\n",
    "            if datapath not in dataset:\n",
    "                dataset[datapath], _ = read_data(datapath)\n",
    "        if 'text2_corpus' in input_conf[tag]:\n",
    "            datapath = input_conf[tag]['text2_corpus']\n",
    "            if datapath not in dataset:\n",
    "                dataset[datapath], _ = read_data(datapath)\n",
    "    print('[Dataset] %s Dataset Load Done.' % len(dataset), end='\\n')\n",
    "\n",
    "    # initial data generator\n",
    "    train_gen = OrderedDict()\n",
    "    eval_gen = OrderedDict()\n",
    "\n",
    "    for tag, conf in input_train_conf.items():\n",
    "        print(conf, end='\\n')\n",
    "        conf['data1'] = dataset[conf['text1_corpus']]\n",
    "        conf['data2'] = dataset[conf['text2_corpus']]\n",
    "        generator = inputs.get(conf['input_type'])\n",
    "        train_gen[tag] = generator( config = conf )\n",
    "\n",
    "    for tag, conf in input_eval_conf.items():\n",
    "        print(conf, end='\\n')\n",
    "        conf['data1'] = dataset[conf['text1_corpus']]\n",
    "        conf['data2'] = dataset[conf['text2_corpus']]\n",
    "        generator = inputs.get(conf['input_type'])\n",
    "        eval_gen[tag] = generator( config = conf )\n",
    "\n",
    "    ######### Load Model #########\n",
    "    model = load_model(config)\n",
    "\n",
    "    loss = []\n",
    "#     loss.append(rank_hinge_loss(config['losses'][0]['object_params']))\n",
    "#     loss.append(hinge_loss_over_sentence(int(config['inputs']['share']['text2_maxlen'] / config['inputs']['share']['text1_maxlen'])))\n",
    "    for lobj in config['losses']:\n",
    "        if lobj['object_name'] in mz_specialized_losses:\n",
    "            loss.append(rank_losses.get(lobj['object_name'])(lobj['object_params']))\n",
    "        else:\n",
    "            loss.append(rank_losses.get(lobj['object_name']))\n",
    "    eval_metrics = OrderedDict()\n",
    "    for mobj in config['metrics']:\n",
    "        mobj = mobj.lower()\n",
    "        if '@' in mobj:\n",
    "            mt_key, mt_val = mobj.split('@', 1)\n",
    "            eval_metrics[mobj] = metrics.get(mt_key)(int(mt_val))\n",
    "        else:\n",
    "            eval_metrics[mobj] = metrics.get(mobj)\n",
    "    optimizer = Adadelta(lr=config[\"global\"][\"learning_rate\"], rho=0.95)\n",
    "    model.compile(optimizer = optimizer, loss=loss)\n",
    "    print('[Model] Model Compile Done.', end='\\n')\n",
    "\n",
    "    for i_e in range(num_iters):\n",
    "        for tag, generator in train_gen.items():\n",
    "            genfun = generator.get_batch_generator()\n",
    "            print('[%s]\\t[Train:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "            history = model.fit_generator(\n",
    "                    genfun,\n",
    "                    steps_per_epoch = display_interval,\n",
    "                    epochs = 1,\n",
    "                    shuffle=False,\n",
    "                    verbose = 0\n",
    "                ) #callbacks=[eval_map])\n",
    "            print('Iter:%d\\tloss=%.6f' % (i_e, history.history['loss'][0]), end='\\n')\n",
    "\n",
    "#         for tag, generator in eval_gen.items():\n",
    "#             genfun = generator.get_batch_generator()\n",
    "#             print('[%s]\\t[Eval:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "#             res = dict([[k,0.] for k in eval_metrics.keys()])\n",
    "#             num_valid = 0\n",
    "#             for input_data, y_true in genfun:\n",
    "#                 y_pred = model.predict(input_data, batch_size=len(y_true))\n",
    "#                 if issubclass(type(generator), inputs.list_generator.ListBasicGenerator):\n",
    "#                     list_counts = input_data['list_counts']\n",
    "#                     for k, eval_func in eval_metrics.items():\n",
    "#                         for lc_idx in range(len(list_counts)-1):\n",
    "#                             pre = list_counts[lc_idx]\n",
    "#                             suf = list_counts[lc_idx+1]\n",
    "#                             res[k] += eval_func(y_true = y_true[pre:suf], y_pred = y_pred[pre:suf])\n",
    "#                     num_valid += len(list_counts) - 1\n",
    "#                 else:\n",
    "#                     for k, eval_func in eval_metrics.items():\n",
    "#                         res[k] += eval_func(y_true = y_true, y_pred = y_pred)\n",
    "#                     num_valid += 1\n",
    "#             generator.reset()\n",
    "#             print('Iter:%d\\t%s' % (i_e, '\\t'.join(['%s=%f'%(k,v/num_valid) for k, v in res.items()])), end='\\n')\n",
    "#             sys.stdout.flush()\n",
    "        if (i_e+1) % save_weights_iters == 0:\n",
    "            model.save_weights(weights_file % (i_e+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "  \"net_name\": \"MYMODEL\",\n",
    "  \"global\":{\n",
    "      \"model_type\": \"PY\",\n",
    "      \"weights_file\": \"../examples/pinfo/weights/mymodel_gru.pinfo.weights\",\n",
    "      \"save_weights_iters\": 10,\n",
    "      \"num_iters\": 1000,\n",
    "      \"display_interval\": 10,\n",
    "      \"test_weights_iters\": 1000,\n",
    "      \"optimizer\": \"adadelta\",\n",
    "      \"learning_rate\": 2.0\n",
    "  },\n",
    "  \"inputs\": {\n",
    "    \"share\": {\n",
    "        \"text1_corpus\": \"../data/pinfo/corpus_preprocessed.txt\",\n",
    "        \"text2_corpus\": \"../data/pinfo/corpus_preprocessed.txt\",\n",
    "        \"use_dpool\": False,\n",
    "        \"embed_size\": 50,\n",
    "        \"embed_path\": \"../data/pinfo/embed_glove_d50\",\n",
    "        \"vocab_size\": 17601,\n",
    "        \"train_embed\": False,\n",
    "        \"target_mode\": \"ranking\",\n",
    "        \"text1_maxlen\": 15,\n",
    "        \"text2_maxlen\": 300\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"input_type\": \"PairGenerator\", \n",
    "        \"phase\": \"TRAIN\",\n",
    "        \"use_iter\": False,\n",
    "        \"query_per_iter\": 2,\n",
    "        \"batch_per_iter\": 1,\n",
    "        \"batch_size\": 20,\n",
    "        \"relation_file\": \"../data/pinfo/relation_train.txt\"\n",
    "    },\n",
    "    \"valid\": {\n",
    "        \"input_type\": \"ListGenerator\", \n",
    "        \"phase\": \"EVAL\",\n",
    "        \"batch_list\": 10,\n",
    "        \"relation_file\": \"../data/pinfo/relation_valid.txt\"\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"input_type\": \"ListGenerator\", \n",
    "        \"phase\": \"EVAL\",\n",
    "        \"batch_list\": 10,\n",
    "        \"relation_file\": \"../data/pinfo/relation_test.txt\"\n",
    "    },\n",
    "    \"predict\": {\n",
    "        \"input_type\": \"ListGenerator\", \n",
    "        \"phase\": \"PREDICT\",\n",
    "        \"batch_list\": 10,\n",
    "        \"relation_file\": \"../data/pinfo/relation_test.txt\"\n",
    "    }\n",
    "  },\n",
    "  \"outputs\": {\n",
    "    \"predict\": {\n",
    "      \"save_format\": \"TREC\",\n",
    "      \"save_path\": \"predict.test.mymodel_gru.pinfo.txt\"\n",
    "    }\n",
    "  },\n",
    "  \"model\": {\n",
    "    \"model_path\": \"./matchzoo/models/\",\n",
    "    \"model_py\": \"mymodel_gru.MYMODEL\",\n",
    "    \"setting\": {\n",
    "        \"hidden_size\": 150,\n",
    "        \"topk\": 100,\n",
    "        \"dropout_rate\": 0.2\n",
    "    }\n",
    "  },\n",
    "  \"losses\": [ \n",
    "    {\n",
    "       \"object_name\": \"rank_hinge_loss\",\n",
    "       \"object_params\": {\n",
    "            \"margin\": 1.0\n",
    "       }\n",
    "    },\n",
    "    {\n",
    "        \"object_name\": \"hinge_loss_over_sentence\" ,\n",
    "        \"object_params\": {\n",
    "            \"margin\": 20.0\n",
    "        }\n",
    "    }\n",
    "  ],\n",
    "  \"metrics\": [ \"ndcg@3\", \"ndcg@5\", \"ndcg@10\", \"map\", \"recall@1\", \"recall@3\", \"recall@5\"]\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"net_name\": \"MYMODEL\",\n",
      "  \"global\": {\n",
      "    \"model_type\": \"PY\",\n",
      "    \"weights_file\": \"../examples/pinfo/weights/mymodel_gru.pinfo.weights\",\n",
      "    \"save_weights_iters\": 10,\n",
      "    \"num_iters\": 1000,\n",
      "    \"display_interval\": 10,\n",
      "    \"test_weights_iters\": 1000,\n",
      "    \"optimizer\": \"adadelta\",\n",
      "    \"learning_rate\": 2.0\n",
      "  },\n",
      "  \"inputs\": {\n",
      "    \"share\": {\n",
      "      \"text1_corpus\": \"../data/pinfo/corpus_preprocessed.txt\",\n",
      "      \"text2_corpus\": \"../data/pinfo/corpus_preprocessed.txt\",\n",
      "      \"use_dpool\": false,\n",
      "      \"embed_size\": 50,\n",
      "      \"embed_path\": \"../data/pinfo/embed_glove_d50\",\n",
      "      \"vocab_size\": 17601,\n",
      "      \"train_embed\": false,\n",
      "      \"target_mode\": \"ranking\",\n",
      "      \"text1_maxlen\": 15,\n",
      "      \"text2_maxlen\": 300\n",
      "    },\n",
      "    \"train\": {\n",
      "      \"input_type\": \"PairGenerator\",\n",
      "      \"phase\": \"TRAIN\",\n",
      "      \"use_iter\": false,\n",
      "      \"query_per_iter\": 2,\n",
      "      \"batch_per_iter\": 1,\n",
      "      \"batch_size\": 20,\n",
      "      \"relation_file\": \"../data/pinfo/relation_train.txt\"\n",
      "    },\n",
      "    \"valid\": {\n",
      "      \"input_type\": \"ListGenerator\",\n",
      "      \"phase\": \"EVAL\",\n",
      "      \"batch_list\": 10,\n",
      "      \"relation_file\": \"../data/pinfo/relation_valid.txt\"\n",
      "    },\n",
      "    \"test\": {\n",
      "      \"input_type\": \"ListGenerator\",\n",
      "      \"phase\": \"EVAL\",\n",
      "      \"batch_list\": 10,\n",
      "      \"relation_file\": \"../data/pinfo/relation_test.txt\"\n",
      "    },\n",
      "    \"predict\": {\n",
      "      \"input_type\": \"ListGenerator\",\n",
      "      \"phase\": \"PREDICT\",\n",
      "      \"batch_list\": 10,\n",
      "      \"relation_file\": \"../data/pinfo/relation_test.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"outputs\": {\n",
      "    \"predict\": {\n",
      "      \"save_format\": \"TREC\",\n",
      "      \"save_path\": \"predict.test.mymodel_gru.pinfo.txt\"\n",
      "    }\n",
      "  },\n",
      "  \"model\": {\n",
      "    \"model_path\": \"./matchzoo/models/\",\n",
      "    \"model_py\": \"mymodel_gru.MYMODEL\",\n",
      "    \"setting\": {\n",
      "      \"hidden_size\": 150,\n",
      "      \"topk\": 100,\n",
      "      \"dropout_rate\": 0.2\n",
      "    }\n",
      "  },\n",
      "  \"losses\": [\n",
      "    {\n",
      "      \"object_name\": \"rank_hinge_loss\",\n",
      "      \"object_params\": {\n",
      "        \"margin\": 1.0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"object_name\": \"hinge_loss_over_sentence\",\n",
      "      \"object_params\": {\n",
      "        \"margin\": 1.0\n",
      "      }\n",
      "    }\n",
      "  ],\n",
      "  \"metrics\": [\n",
      "    \"ndcg@3\",\n",
      "    \"ndcg@5\",\n",
      "    \"ndcg@10\",\n",
      "    \"map\",\n",
      "    \"recall@1\",\n",
      "    \"recall@3\",\n",
      "    \"recall@5\"\n",
      "  ]\n",
      "}\n",
      "[../data/pinfo/embed_glove_d50]\n",
      "\tEmbedding size: 17601\n",
      "Generate numpy embed: (17601, 50)\n",
      "[Embedding] Embedding Load Done.\n",
      "[Input] Process Input Tags. odict_keys(['train']) in TRAIN, odict_keys(['valid', 'test']) in EVAL.\n",
      "[../data/pinfo/corpus_preprocessed.txt]\n",
      "\tData size: 19772\n",
      "[Dataset] 1 Dataset Load Done.\n",
      "{'text1_corpus': '../data/pinfo/corpus_preprocessed.txt', 'text2_corpus': '../data/pinfo/corpus_preprocessed.txt', 'use_dpool': False, 'embed_size': 50, 'embed_path': '../data/pinfo/embed_glove_d50', 'vocab_size': 17601, 'train_embed': False, 'target_mode': 'ranking', 'text1_maxlen': 15, 'text2_maxlen': 300, 'embed': array([[ 0.45323   ,  0.059811  , -0.10577   , ...,  0.5324    ,\n",
      "        -0.25103   ,  0.62546   ],\n",
      "       [ 1.1767    ,  0.52921   , -0.0018451 , ..., -0.07421   ,\n",
      "         0.24272   ,  0.102     ],\n",
      "       [ 0.16113   , -0.1652    , -0.7074    , ...,  0.24979   ,\n",
      "         0.47837   , -0.042246  ],\n",
      "       ...,\n",
      "       [ 0.1741781 , -0.10418008, -0.1002242 , ..., -0.04712802,\n",
      "        -0.04958404, -0.16864365],\n",
      "       [ 0.08615811, -0.04204081,  0.07595906, ...,  0.13597283,\n",
      "        -0.09083351, -0.0554575 ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32), 'input_type': 'PairGenerator', 'phase': 'TRAIN', 'use_iter': False, 'query_per_iter': 2, 'batch_per_iter': 1, 'batch_size': 20, 'relation_file': '../data/pinfo/relation_train.txt'}\n",
      "deserialize PairGenerator\n",
      "[../data/pinfo/relation_train.txt]\n",
      "\tInstance size: 52740\n",
      "Pair Instance Count: 47466\n",
      "[PairGenerator] init done\n",
      "{'text1_corpus': '../data/pinfo/corpus_preprocessed.txt', 'text2_corpus': '../data/pinfo/corpus_preprocessed.txt', 'use_dpool': False, 'embed_size': 50, 'embed_path': '../data/pinfo/embed_glove_d50', 'vocab_size': 17601, 'train_embed': False, 'target_mode': 'ranking', 'text1_maxlen': 15, 'text2_maxlen': 300, 'embed': array([[ 0.45323   ,  0.059811  , -0.10577   , ...,  0.5324    ,\n",
      "        -0.25103   ,  0.62546   ],\n",
      "       [ 1.1767    ,  0.52921   , -0.0018451 , ..., -0.07421   ,\n",
      "         0.24272   ,  0.102     ],\n",
      "       [ 0.16113   , -0.1652    , -0.7074    , ...,  0.24979   ,\n",
      "         0.47837   , -0.042246  ],\n",
      "       ...,\n",
      "       [ 0.1741781 , -0.10418008, -0.1002242 , ..., -0.04712802,\n",
      "        -0.04958404, -0.16864365],\n",
      "       [ 0.08615811, -0.04204081,  0.07595906, ...,  0.13597283,\n",
      "        -0.09083351, -0.0554575 ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32), 'input_type': 'ListGenerator', 'phase': 'EVAL', 'batch_list': 10, 'relation_file': '../data/pinfo/relation_valid.txt'}\n",
      "deserialize ListGenerator\n",
      "[../data/pinfo/relation_valid.txt]\n",
      "\tInstance size: 8013\n",
      "List Instance Count: 508\n",
      "[ListGenerator] init done\n",
      "{'text1_corpus': '../data/pinfo/corpus_preprocessed.txt', 'text2_corpus': '../data/pinfo/corpus_preprocessed.txt', 'use_dpool': False, 'embed_size': 50, 'embed_path': '../data/pinfo/embed_glove_d50', 'vocab_size': 17601, 'train_embed': False, 'target_mode': 'ranking', 'text1_maxlen': 15, 'text2_maxlen': 300, 'embed': array([[ 0.45323   ,  0.059811  , -0.10577   , ...,  0.5324    ,\n",
      "        -0.25103   ,  0.62546   ],\n",
      "       [ 1.1767    ,  0.52921   , -0.0018451 , ..., -0.07421   ,\n",
      "         0.24272   ,  0.102     ],\n",
      "       [ 0.16113   , -0.1652    , -0.7074    , ...,  0.24979   ,\n",
      "         0.47837   , -0.042246  ],\n",
      "       ...,\n",
      "       [ 0.1741781 , -0.10418008, -0.1002242 , ..., -0.04712802,\n",
      "        -0.04958404, -0.16864365],\n",
      "       [ 0.08615811, -0.04204081,  0.07595906, ...,  0.13597283,\n",
      "        -0.09083351, -0.0554575 ],\n",
      "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
      "         0.        ,  0.        ]], dtype=float32), 'input_type': 'ListGenerator', 'phase': 'EVAL', 'batch_list': 10, 'relation_file': '../data/pinfo/relation_test.txt'}\n",
      "deserialize ListGenerator\n",
      "[../data/pinfo/relation_test.txt]\n",
      "\tInstance size: 13958\n",
      "List Instance Count: 936\n",
      "[ListGenerator] init done\n",
      "[MYMODEL] init done\n",
      "[layer]: Input\t[shape]: [None, 15] \n",
      "66.0% memory has been used\n",
      "[layer]: Input\t[shape]: [None, 300] \n",
      "66.0% memory has been used\n",
      "[layer]: Embedding\t[shape]: [None, 15, 50] \n",
      "66.1% memory has been used\n",
      "[layer]: Embedding\t[shape]: [None, 15, 50] \n",
      "66.1% memory has been used\n",
      "[layer]: Bidirectional-LSTM\t[shape]: [None, None, 300] \n",
      "66.3% memory has been used\n",
      "[layer]: Bidirectional-LSTM\t[shape]: [None, None, 300] \n",
      "66.4% memory has been used\n",
      "[layer]: context2query_attention\t[shape]: [None, 15, 1200] \n",
      "66.4% memory has been used\n",
      "input_shape (None, 15, 1200)\n",
      "[layer]: att 1\t[shape]: [None, 1200] \n",
      "66.4% memory has been used\n",
      "[layer]: exp 1\t[shape]: [None, 1, 15] \n",
      "66.4% memory has been used\n",
      "[layer]: tile 1\t[shape]: [None, 20, 15] \n",
      "66.4% memory has been used\n",
      "[layer]: query4_s\t[shape]: [None, 20, 15] \n",
      "66.4% memory has been used\n",
      "[layer]: doc4\t[shape]: [None, 20, 15] \n",
      "66.4% memory has been used\n",
      "[layer]: concat 1\t[shape]: [None, 20, 30] \n",
      "66.4% memory has been used\n",
      "input_shape (None, 20, 1200)\n",
      "[layer]: att 2\t[shape]: [None, 1200] \n",
      "66.5% memory has been used\n",
      "input_shape (None, 15, 300)\n",
      "[layer]: att q\t[shape]: [None, 300] \n",
      "66.5% memory has been used\n",
      "[layer]: Reshape\t[shape]: [None, None] \n",
      "66.5% memory has been used\n",
      "[layer]: Dropout\t[shape]: [None, None] \n",
      "66.5% memory has been used\n",
      "[layer]: Dense\t[shape]: [None, 1] \n",
      "66.5% memory has been used\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'dict' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-76f5d0fd4c5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'predict'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-dbf839a7d2b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0meval_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmobj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdadelta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"global\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"learning_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.95\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'[Model] Model Compile Done.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    828\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m                     output_loss = weighted_loss(y_true, y_pred,\n\u001b[0;32m--> 830\u001b[0;31m                                                 sample_weight, mask)\n\u001b[0m\u001b[1;32m    831\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \"\"\"\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/MatchZoo-master/matchzoo/losses/rank_losses.py\u001b[0m in \u001b[0;36m_margin_loss_sent\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# sentence loss only over positive samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0my_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnum_sent\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0my_neg_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mshow_layer_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'y pos'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'dict' and 'int'"
     ]
    }
   ],
   "source": [
    "phase = 'train'\n",
    "# model_file = '../examples/wikiqa/config/mvlstm_wikiqa.config'\n",
    "# with open(model_file, 'r') as f:\n",
    "#     config = json.load(f)\n",
    "    \n",
    "if phase == 'train':\n",
    "    train(config)\n",
    "elif phase == 'predict':\n",
    "    predict(config)\n",
    "else:\n",
    "    print('Phase Error.', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
