{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mingzhu/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf8 -*-\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import argparse\n",
    "import random\n",
    "random.seed(49999)\n",
    "import numpy\n",
    "numpy.random.seed(49999)\n",
    "import tensorflow\n",
    "tensorflow.set_random_seed(49999)\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from utils import *\n",
    "import inputs\n",
    "import metrics\n",
    "from losses import *\n",
    "from optimizers import *\n",
    "\n",
    "config = tensorflow.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tensorflow.Session(config = config)\n",
    "\n",
    "def load_model(config):\n",
    "    global_conf = config[\"global\"]\n",
    "    model_type = global_conf['model_type']\n",
    "    if model_type == 'JSON':\n",
    "        mo = Model.from_config(config['model'])\n",
    "    elif model_type == 'PY':\n",
    "        model_config = config['model']['setting']\n",
    "        model_config.update(config['inputs']['share'])\n",
    "        sys.path.insert(0, config['model']['model_path'])\n",
    "\n",
    "        model = import_object(config['model']['model_py'], model_config)\n",
    "        mo = model.build()\n",
    "    return mo\n",
    "\n",
    "\n",
    "def train(config):\n",
    "\n",
    "    print(json.dumps(config, indent=2), end='\\n')\n",
    "    # read basic config\n",
    "    global_conf = config[\"global\"]\n",
    "    optimizer = global_conf['optimizer']\n",
    "    optimizer=optimizers.get(optimizer)\n",
    "    K.set_value(optimizer.lr, global_conf['learning_rate'])\n",
    "    weights_file = str(global_conf['weights_file']) + '.%d'\n",
    "    display_interval = int(global_conf['display_interval'])\n",
    "    num_iters = int(global_conf['num_iters'])\n",
    "    save_weights_iters = int(global_conf['save_weights_iters'])\n",
    "\n",
    "    # read input config\n",
    "    input_conf = config['inputs']\n",
    "    share_input_conf = input_conf['share']\n",
    "\n",
    "\n",
    "    # collect embedding\n",
    "    if 'embed_path' in share_input_conf:\n",
    "        embed_dict = read_embedding(filename=share_input_conf['embed_path'])\n",
    "        _PAD_ = share_input_conf['vocab_size'] - 1\n",
    "        embed_dict[_PAD_] = np.zeros((share_input_conf['embed_size'], ), dtype=np.float32)\n",
    "        embed = np.float32(np.random.uniform(-0.2, 0.2, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = convert_embed_2_numpy(embed_dict, embed = embed)\n",
    "    else:\n",
    "        # if no embed provided, use random\n",
    "        embed = np.float32(np.random.uniform(-0.2, 0.2, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = embed\n",
    "    print('[Embedding] Embedding Load Done.', end='\\n')\n",
    "\n",
    "    # list all input tags and construct tags config\n",
    "    input_train_conf = OrderedDict()\n",
    "    input_eval_conf = OrderedDict()\n",
    "    print(\"input_conf\", input_conf)\n",
    "    print(\"input_conf keys\", input_conf.keys())\n",
    "    for tag in input_conf.keys():\n",
    "        if 'phase' not in input_conf[tag]:\n",
    "            continue\n",
    "        if input_conf[tag]['phase'] == 'TRAIN':\n",
    "            input_train_conf[tag] = {}\n",
    "            input_train_conf[tag].update(share_input_conf)\n",
    "            input_train_conf[tag].update(input_conf[tag])\n",
    "        elif input_conf[tag]['phase'] == 'EVAL':\n",
    "            input_eval_conf[tag] = {}\n",
    "            input_eval_conf[tag].update(share_input_conf)\n",
    "            input_eval_conf[tag].update(input_conf[tag])\n",
    "    print('[Input] Process Input Tags. %s in TRAIN, %s in EVAL.' % (input_train_conf.keys(), input_eval_conf.keys()), end='\\n')\n",
    "    print(\"input_train_conf\", input_train_conf)\n",
    "    # collect dataset identification\n",
    "    dataset = {}\n",
    "    for tag in input_conf:\n",
    "        if tag != 'share' and input_conf[tag]['phase'] == 'PREDICT':\n",
    "            continue\n",
    "        if 'text1_corpus' in input_conf[tag]:\n",
    "            datapath = input_conf[tag]['text1_corpus']\n",
    "            if datapath not in dataset:\n",
    "                dataset[datapath], _ = read_data(datapath)\n",
    "        if 'text2_corpus' in input_conf[tag]:\n",
    "            datapath = input_conf[tag]['text2_corpus']\n",
    "            if datapath not in dataset:\n",
    "                dataset[datapath], _ = read_data(datapath)\n",
    "    print('[Dataset] %s Dataset Load Done.' % len(dataset), end='\\n')\n",
    "\n",
    "    # initial data generator\n",
    "    train_gen = OrderedDict()\n",
    "    eval_gen = OrderedDict()\n",
    "\n",
    "    #pair generator is for data with label, list generator is for data without label\n",
    "    for tag, conf in input_train_conf.items():\n",
    "        print(conf, end='\\n')\n",
    "        conf['data1'] = dataset[conf['text1_corpus']]\n",
    "        conf['data2'] = dataset[conf['text2_corpus']]\n",
    "        generator = inputs.get(conf['input_type'])\n",
    "        train_gen[tag] = generator( config = conf )\n",
    "\n",
    "    for tag, conf in input_eval_conf.items():\n",
    "        print(conf, end='\\n')\n",
    "        conf['data1'] = dataset[conf['text1_corpus']]\n",
    "        conf['data2'] = dataset[conf['text2_corpus']]\n",
    "        generator = inputs.get(conf['input_type'])\n",
    "        eval_gen[tag] = generator( config = conf )\n",
    "\n",
    "    ######### Load Model #########\n",
    "    model = load_model(config)\n",
    "\n",
    "    loss = []\n",
    "    for lobj in config['losses']:\n",
    "        if lobj['object_name'] in mz_specialized_losses:\n",
    "            loss.append(rank_losses.get(lobj['object_name'])(lobj['object_params']))\n",
    "        else:\n",
    "            loss.append(rank_losses.get(lobj['object_name']))\n",
    "    eval_metrics = OrderedDict()\n",
    "    for mobj in config['metrics']:\n",
    "        mobj = mobj.lower()\n",
    "        if '@' in mobj:\n",
    "            mt_key, mt_val = mobj.split('@', 1)\n",
    "            eval_metrics[mobj] = metrics.get(mt_key)(int(mt_val))\n",
    "        else:\n",
    "            eval_metrics[mobj] = metrics.get(mobj)\n",
    "    model.compile(optimizer=optimizer, loss=loss)\n",
    "    print('[Model] Model Compile Done.', end='\\n')\n",
    "\n",
    "    for i_e in range(num_iters):\n",
    "        for tag, generator in train_gen.items():\n",
    "            genfun = generator.get_batch_generator()\n",
    "            print('[%s]\\t[Train:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "            history = model.fit_generator(\n",
    "                    genfun,\n",
    "                    steps_per_epoch = display_interval,\n",
    "                    epochs = 1,\n",
    "                    shuffle=False,\n",
    "                    verbose = 0\n",
    "                ) #callbacks=[eval_map])\n",
    "            print(\"history.history\", history.history)\n",
    "            print('Iter:%d\\tloss=%.6f' % (i_e, history.history['loss'][0]), end='\\n')\n",
    "\n",
    "        for tag, generator in eval_gen.items():\n",
    "            genfun = generator.get_batch_generator()\n",
    "            print('[%s]\\t[Eval:%s] ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "            res = dict([[k,0.] for k in eval_metrics.keys()])\n",
    "            num_valid = 0\n",
    "            for input_data, y_true in genfun:\n",
    "                print(\"len y_true\", len(y_true))\n",
    "                y_pred = model.predict(input_data, batch_size=len(y_true))\n",
    "                if issubclass(type(generator), inputs.list_generator.ListBasicGenerator):\n",
    "                    list_counts = input_data['list_counts']\n",
    "                    for k, eval_func in eval_metrics.items():\n",
    "                        for lc_idx in range(len(list_counts)-1):\n",
    "                            pre = list_counts[lc_idx]\n",
    "                            suf = list_counts[lc_idx+1]\n",
    "                            print(\"pre\", pre)\n",
    "                            print(\"suf\", suf)\n",
    "                            res[k] += eval_func(y_true = y_true[pre:suf], y_pred = y_pred[pre:suf])\n",
    "                    num_valid += len(list_counts) - 1\n",
    "                else:\n",
    "                    for k, eval_func in eval_metrics.items():\n",
    "                        res[k] += eval_func(y_true = y_true, y_pred = y_pred)\n",
    "                    num_valid += 1\n",
    "            generator.reset()\n",
    "            print('Iter:%d\\t%s' % (i_e, '\\t'.join(['%s=%f'%(k,v/num_valid) for k, v in res.items()])), end='\\n')\n",
    "            sys.stdout.flush()\n",
    "        if (i_e+1) % save_weights_iters == 0:\n",
    "            model.save_weights(weights_file % (i_e+1))\n",
    "\n",
    "def predict(config):\n",
    "    ######## Read input config ########\n",
    "\n",
    "    print(json.dumps(config, indent=2), end='\\n')\n",
    "    input_conf = config['inputs']\n",
    "    share_input_conf = input_conf['share']\n",
    "\n",
    "    # collect embedding\n",
    "    if 'embed_path' in share_input_conf:\n",
    "        embed_dict = read_embedding(filename=share_input_conf['embed_path'])\n",
    "        _PAD_ = share_input_conf['vocab_size'] - 1\n",
    "        embed_dict[_PAD_] = np.zeros((share_input_conf['embed_size'], ), dtype=np.float32)\n",
    "        embed = np.float32(np.random.uniform(-0.02, 0.02, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = convert_embed_2_numpy(embed_dict, embed = embed)\n",
    "    else:\n",
    "        embed = np.float32(np.random.uniform(-0.2, 0.2, [share_input_conf['vocab_size'], share_input_conf['embed_size']]))\n",
    "        share_input_conf['embed'] = embed\n",
    "    print('[Embedding] Embedding Load Done.', end='\\n')\n",
    "\n",
    "    # list all input tags and construct tags config\n",
    "    input_predict_conf = OrderedDict()\n",
    "    for tag in input_conf.keys():\n",
    "        if 'phase' not in input_conf[tag]:\n",
    "            continue\n",
    "        if input_conf[tag]['phase'] == 'PREDICT':\n",
    "            input_predict_conf[tag] = {}\n",
    "            input_predict_conf[tag].update(share_input_conf)\n",
    "            input_predict_conf[tag].update(input_conf[tag])\n",
    "    print('[Input] Process Input Tags. %s in PREDICT.' % (input_predict_conf.keys()), end='\\n')\n",
    "\n",
    "    # collect dataset identification\n",
    "    dataset = {}\n",
    "    for tag in input_conf:\n",
    "        if tag == 'share' or input_conf[tag]['phase'] == 'PREDICT':\n",
    "            if 'text1_corpus' in input_conf[tag]:\n",
    "                datapath = input_conf[tag]['text1_corpus']\n",
    "                if datapath not in dataset:\n",
    "                    dataset[datapath], _ = read_data(datapath)\n",
    "            if 'text2_corpus' in input_conf[tag]:\n",
    "                datapath = input_conf[tag]['text2_corpus']\n",
    "                if datapath not in dataset:\n",
    "                    dataset[datapath], _ = read_data(datapath)\n",
    "    print('[Dataset] %s Dataset Load Done.' % len(dataset), end='\\n')\n",
    "\n",
    "    # initial data generator\n",
    "    predict_gen = OrderedDict()\n",
    "\n",
    "    for tag, conf in input_predict_conf.items():\n",
    "        print(conf, end='\\n')\n",
    "        conf['data1'] = dataset[conf['text1_corpus']]\n",
    "        conf['data2'] = dataset[conf['text2_corpus']]\n",
    "        generator = inputs.get(conf['input_type'])\n",
    "        predict_gen[tag] = generator(\n",
    "                                    #data1 = dataset[conf['text1_corpus']],\n",
    "                                    #data2 = dataset[conf['text2_corpus']],\n",
    "                                     config = conf )\n",
    "\n",
    "    ######## Read output config ########\n",
    "    output_conf = config['outputs']\n",
    "\n",
    "    ######## Load Model ########\n",
    "    global_conf = config[\"global\"]\n",
    "    weights_file = str(global_conf['weights_file']) + '.' + str(global_conf['test_weights_iters'])\n",
    "\n",
    "    model = load_model(config)\n",
    "    model.load_weights(weights_file)\n",
    "\n",
    "    eval_metrics = OrderedDict()\n",
    "    for mobj in config['metrics']:\n",
    "        mobj = mobj.lower()\n",
    "        if '@' in mobj:\n",
    "            mt_key, mt_val = mobj.split('@', 1)\n",
    "            eval_metrics[mobj] = metrics.get(mt_key)(int(mt_val))\n",
    "        else:\n",
    "            eval_metrics[mobj] = metrics.get(mobj)\n",
    "    res = dict([[k,0.] for k in eval_metrics.keys()])\n",
    "\n",
    "    for tag, generator in predict_gen.items():\n",
    "        genfun = generator.get_batch_generator()\n",
    "        print('[%s]\\t[Predict] @ %s ' % (time.strftime('%m-%d-%Y %H:%M:%S', time.localtime(time.time())), tag), end='')\n",
    "        num_valid = 0\n",
    "        res_scores = {}\n",
    "        for input_data, y_true in genfun:\n",
    "            y_pred = model.predict(input_data, batch_size=len(y_true) )\n",
    "\n",
    "            if issubclass(type(generator), inputs.list_generator.ListBasicGenerator):\n",
    "                list_counts = input_data['list_counts']\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    for lc_idx in range(len(list_counts)-1):\n",
    "                        pre = list_counts[lc_idx]\n",
    "                        suf = list_counts[lc_idx+1]\n",
    "                        res[k] += eval_func(y_true = y_true[pre:suf], y_pred = y_pred[pre:suf])\n",
    "\n",
    "                y_pred = np.squeeze(y_pred)\n",
    "                for lc_idx in range(len(list_counts)-1):\n",
    "                    pre = list_counts[lc_idx]\n",
    "                    suf = list_counts[lc_idx+1]\n",
    "                    for p, y, t in zip(input_data['ID'][pre:suf], y_pred[pre:suf], y_true[pre:suf]):\n",
    "                        if p[0] not in res_scores:\n",
    "                            res_scores[p[0]] = {}\n",
    "                        res_scores[p[0]][p[1]] = (y, t)\n",
    "\n",
    "                num_valid += len(list_counts) - 1\n",
    "            else:\n",
    "                for k, eval_func in eval_metrics.items():\n",
    "                    res[k] += eval_func(y_true = y_true, y_pred = y_pred)\n",
    "                for p, y, t in zip(input_data['ID'], y_pred, y_true):\n",
    "                    if p[0] not in res_scores:\n",
    "                        res_scores[p[0]] = {}\n",
    "                    res_scores[p[0]][p[1]] = (y[1], t[1])\n",
    "                num_valid += 1\n",
    "        generator.reset()\n",
    "\n",
    "        if tag in output_conf:\n",
    "            if output_conf[tag]['save_format'] == 'TREC':\n",
    "                with open(output_conf[tag]['save_path'], 'w') as f:\n",
    "                    for qid, dinfo in res_scores.items():\n",
    "                        dinfo = sorted(dinfo.items(), key=lambda d:d[1][0], reverse=True)\n",
    "                        for inum,(did, (score, gt)) in enumerate(dinfo):\n",
    "                            f.write('%s\\tQ0\\t%s\\t%d\\t%f\\t%s\\t%s\\n'%(qid, did, inum, score, config['net_name'], gt))\n",
    "            elif output_conf[tag]['save_format'] == 'TEXTNET':\n",
    "                with open(output_conf[tag]['save_path'], 'w') as f:\n",
    "                    for qid, dinfo in res_scores.items():\n",
    "                        dinfo = sorted(dinfo.items(), key=lambda d:d[1][0], reverse=True)\n",
    "                        for inum,(did, (score, gt)) in enumerate(dinfo):\n",
    "                            f.write('%s %s %s %s\\n'%(gt, qid, did, score))\n",
    "\n",
    "        print('[Predict] results: ', '\\t'.join(['%s=%f'%(k,v/num_valid) for k, v in res.items()]), end='\\n')\n",
    "        sys.stdout.flush()\n",
    "\n",
    "def main1(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--phase', default='train', help='Phase: Can be train or predict, the default value is train.')\n",
    "    parser.add_argument('--model_file', default='./models/arci.config', help='Model_file: MatchZoo model file for the chosen model.')\n",
    "    args = parser.parse_args()\n",
    "    model_file =  args.model_file\n",
    "    with open(model_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    phase = args.phase\n",
    "    if args.phase == 'train':\n",
    "        train(config)\n",
    "    elif args.phase == 'predict':\n",
    "        predict(config)\n",
    "    else:\n",
    "        print('Phase Error.', end='\\n')\n",
    "    return\n",
    "\n",
    "# if __name__=='__main__':\n",
    "#     main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = '../examples/pinfo/config/matchpyramid_pinfo1.config'\n",
    "model_file =  mf\n",
    "phase = \"train\"\n",
    "with open(model_file, 'r') as f:\n",
    "    config = json.load(f)\n",
    "if phase == 'train':\n",
    "    train(config)\n",
    "elif phase == 'predict':\n",
    "    predict(config)\n",
    "else:\n",
    "    print('Phase Error.', end='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('ha', {1: 2, 3: 4, 2: 2})])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "x = OrderedDict()\n",
    "x[\"ha\"] = {}\n",
    "a = {1:2, 3:4}\n",
    "b = {2:2, 3:4}\n",
    "x[\"ha\"].update(a)\n",
    "x[\"ha\"].update(b)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pair_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0aef2266efed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# X2[:] = fill_word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md2n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0md1_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1_maxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0md2p_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2_maxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md2p\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pair_list' is not defined"
     ]
    }
   ],
   "source": [
    "# for _ in range(config['batch_per_iter']):\n",
    "batch_size = 3\n",
    "data1_maxlen = 5\n",
    "data2_maxlen = 6\n",
    "fill_word = \"haha\"\n",
    "X1 = np.zeros((batch_size*2, data1_maxlen), dtype=np.int32)\n",
    "X1_len = np.zeros((batch_size*2,), dtype=np.int32)\n",
    "X2 = np.zeros((batch_size*2, data2_maxlen), dtype=np.int32)\n",
    "X2_len = np.zeros((batch_size*2,), dtype=np.int32)\n",
    "Y = np.zeros((batch_size*2,), dtype=np.int32)\n",
    "\n",
    "Y[::2] = 1\n",
    "# X1[:] = fill_word\n",
    "# X2[:] = fill_word\n",
    "for i in range(batch_size):\n",
    "    d1, d2p, d2n = random.choice(pair_list)\n",
    "    d1_len = min(data1_maxlen, len(list(data1[d1])))\n",
    "    d2p_len = min(data2_maxlen, len(list(data2[d2p])))\n",
    "    d2n_len = min(data2_maxlen, len(list(data2[d2n])))\n",
    "    X1[i*2,   :d1_len],  X1_len[i*2]   = data1[d1][:d1_len],   d1_len\n",
    "    X2[i*2,   :d2p_len], X2_len[i*2]   = data2[d2p][:d2p_len], d2p_len\n",
    "    X1[i*2+1, :d1_len],  X1_len[i*2+1] = data1[d1][:d1_len],   d1_len\n",
    "    X2[i*2+1, :d2n_len], X2_len[i*2+1] = data2[d2n][:d2n_len], d2n_len\n",
    "print(X1, X2, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pair_static(rel):\n",
    "    rel_set = {}\n",
    "    pair_list = []\n",
    "    for label, d1, d2 in rel:\n",
    "        if d1 not in rel_set:\n",
    "            rel_set[d1] = {}\n",
    "        if label not in rel_set[d1]:\n",
    "            rel_set[d1][label] = []\n",
    "        rel_set[d1][label].append(d2)\n",
    "    for d1 in rel_set:\n",
    "        label_list = sorted(rel_set[d1].keys(), reverse = True)\n",
    "        for hidx, high_label in enumerate(label_list[:-1]):\n",
    "            for low_label in label_list[hidx+1:]:\n",
    "                for high_d2 in rel_set[d1][high_label]:\n",
    "                    for low_d2 in rel_set[d1][low_label]:\n",
    "                        pair_list.append( (d1, high_d2, low_d2) )\n",
    "    print('Pair Instance Count:', len(pair_list), end='\\n')\n",
    "    return pair_list\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 4], [0, 1, 3], [0, 1, 2], [0, 1, 5]]\n"
     ]
    }
   ],
   "source": [
    "rel = [[1, 1, 4],[0, 1, 3],[0, 1, 2],[0, 1, 5]]\n",
    "rel[0][0] = 1\n",
    "rel[1][0] = 0\n",
    "rel[2][0] = 0\n",
    "print(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair Instance Count: 3\n",
      "[(1, 4, 3), (1, 4, 2), (1, 4, 5)]\n"
     ]
    }
   ],
   "source": [
    "a = make_pair_static(rel)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
